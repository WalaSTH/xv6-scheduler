# Introduction to the Laboratory of Operating Systems

### Group:

Virtualized, named after the OS technique of virtualization.

### Members:

- Leonardo Torres, (leo.torres@mi.unc.edu.ar)
- MatÃ­as Scantamburlo, (matias.scantamburlo@mi.unc.edu.ar)
- Maciel Salguero, (maciel.salguero@mi.unc.edu.ar)

# Developement

In this laboratory, work was done to solve and develop responses to multiple statements segmented into 4 stages:

1. Comprehension of xv6 code to answer characteristic questions about the system.
2. Extending the study of xv6 by observing the different reactions that occurred when modifying the quantum and testing it with the tests given in the course.
3. Beginning to modify the standard xv6 scheduler to MLFQ. To do this, and before anything else, we had to implement the priorities of the programs to be executed.
4. Implementation of the MLFQ scheduler.

Below, we will describe what was done in each task, the problems that arose, and the solutions we gave them.

## First Part

### Exercise 1:

This first part was more about reading and understanding, so it wasn't very difficult for us. Among the three of us, we studied the code of the `scheduler()` function in the **proc.c** file. Analyzing this function, we encountered many things that were unfamiliar to us, so we tried to focus on the more conceptual part of the algorithm without getting lost in the functions and structures we didn't know. We started by studying the ptable structure, which contains all the processes of the operating system. Understanding this was fundamental because it allowed us to see that the second `for` loop effectively goes through each process, searching for one that is in the RUNNABLE state. Then, different context switch functions are used. However, it was unnecessary for us to understand them 100% due to a comment after the last context switch:

```
switchkvm();

// Process is done running for now.
// It should have changed its p->state before coming back.
```

This led us to understand that what the algorithm does is choose a process in the RUNNABLE state, executes it for a period of time, and then releases it. From this, we concluded that the scheduler used by XV6 is a Round Robin (RR) scheduler.

### Exercise 2:

We attempted to find the quantum through several paths, but the one that led us to the desired result was searching for the `yield()` function using the `grep -r` tool in bash. We searched for `yield()` because we know, thanks to the attached image, that this function allows processes to be preempted, so we guessed that the quantum was involved in this process.

![XV6 FSA](https://media.discordapp.net/attachments/882386883643056129/897971286628761651/unknown.png?width=418&height=373)

This led us to the **trap.c** directory, where we found the line of code:

```
// Force process to give up CPU on clock tick.
// If interrupts were on while locks held, would need to check nlock.
if(myproc() && myproc()->state == RUNNING &&
   tf->trapno == T_IRQ0+IRQ_TIMER)
  yield();
```

Subsequently, we traced values **T_IRQ0** and **IRQ_TIMER** to finally reach the **lapic.c** directory. We focused on studying and understanding it, specifically on line 69, where the call is made:

```
lapicw(TICR, 10000000)
```

Without fully understanding what this does, we found where to modify the quantum, either directly or indirectly. To answer the question "how long does the quantum last in xv6?" we had to again study the code, leading us to the conclusion that a timer interrupt is the interruption made by the operating system every period of time to gain control of the CPU. We could say that a quantum is the time interval between two timer interrupts, where this interval is expressed in Ticks. With this, we could better understand the `lapicw()` function and know that the quantum lasts ten million Ticks. If we wanted to alter this time, we would simply change the default value of the `lapicw()` function.

## Second Part

### About the Experiments

The experiments consisted of running programs provided by the course under different scenarios and situations. The way these experiments were conducted was to run each case for a period of 4 minutes, respecting the following:

- Run the programs always on the same computer.
- Always run with the same context used in the first measurement (i.e., open programs, processes, connections, etc.).
- Emulate qemu with a single processor.

All metrics generated by the programs were redirected to text files within xv6 so that they could be manipulated.

### How Was the Information Organized?

During this laboratory, it was necessary to manipulate and work with many data generated from the different experiments that needed to be carried out. Therefore, one of the main challenges of this work was to plan how we were going to organize, compare, and display all this information. Below, we will detail what we opted for in this project.

As a central decision, based on the dataset, we chose that the best way to compare, analyze, and share the results would be using bar charts. Before describing the charts, it should be clarified that the metrics that make up the charts are an average of the data

### General Description of the Charts:

Clarification on Terminology: In this report, we refer to each different execution of the programs under experimentation as a "case," meaning the executions n cpubench; m iobench, with n,m integers in the closed interval \[0,2\]. And we call a "scenario" the realization of these cases with different quantums.

On the x-axis, we have the different experimental cases.
On the y-axis, we find the unit of measurement (which can be KFPT or IOPT).
In each case, there are different bars distinguished by color, representing the average value of the respective case but in different scenarios.
Each of these graphs is done per scheduler (RR, MLFQ).

Because we work with 2 different units of measurement, each of the scheduler graphs had to be separated into 2: one where Kilo Flops Per Tick is on the y-axis, and the other where IO Per Tick is on the y-axis.

Why did we choose this format? We believe that this data organization allows for a rich comparison of the results, whether the comparison is between cases, scenarios, and/or schedulers. Additionally, it facilitates the overall analysis of all the data, allows for the use of all collected information, and significantly reduces the number of necessary graphs.

### About the Automation of the Charts

Next, we will briefly describe how the script that manipulates the information and generates the graphs was developed.
The programming language used for this task was Python, chosen for its ease of use and the availability of libraries for this type of task.
The first step after conducting the experiments was to copy and paste the results from the .txt files into .csv files outside of xv6. We chose this file format because of how the bench outputs are structured. If we use the space (' ') as the delimiter, then obtaining the metrics we are interested in becomes much easier.
Once we had this data in .csv format, it was straightforward to manipulate it in Python and obtain the average of each one.
Within this program, it was decided to use Python's dictionary data structure, where a dictionary represents a specific scenario n, with n being an integer in \[0,3\], and each dictionary element is a case of experimentation, with its value being the respective average.
The process that performs this transformation (from .csv to dictionaries) is the function

**_PRE: It is assumed that the directory names match the pre-established pattern_**
`readScenary(esPath,form)`

Where:

- `esPath` is a string representing the relative path of the scenario to be obtained. For example: "Scenario0".
- `form` is a char representing the type of experiment we want (CPU or IO) and can take values 'c' or 'i'.

As mentioned earlier, the function assumes that the file names comply with a certain pattern, here is the required pattern:

"Casoi_f-n-m.csv" where:

- `i` represents the case and is an integer in the interval \[0,7\].
- `f` represents the type of experiment, which can be 'c' or 'i'.
- `n` and `m` are integers representing the case and the ID of the program (in case more than one cpubench and/or iobench are executed).

For example: "Case5_i-5-2.csv" represents case 5, where iobench is the second program executed.

_The format must be strictly adhered to, as the script DOES NOT use regular expressions._

This function enters the directory passed to it, searches for all .csv files that match the pre-established name pattern, averages those that belong to the same case, and then adds them to the returned dictionary. Finally, the function that generates the graphs is `makeChart(esPath, form)`, which uses the previous function to create dictionaries and then uses the matplotlib module to generate the graphs.

---

Next, we will analyze and compare all the data contained and draw conclusions.

### Analysis of IO Graph

![IOBENCH Metrics RR](https://media.discordapp.net/attachments/882386883643056129/902297867371282442/unknown.png?width=697&height=397)

The best results in all different scenarios are always given by case 0 (1 iobench) and case 7 (2 iobench). It is very interesting to observe that when comparing these 2 cases, it is evident that when RR works with the default quantum, the clear winner is the case that executes only one iobench. However, as we decrease the quantum, they start to have similar results until in the last scenario with the smallest quantum, case 7 performs better.

The worst results are observed in all cases where there is at least one cpubench. The graphs show a significant change compared to the other cases. This occurs because when we run the cases with cpubench, these programs always consume their entire quantum, and therefore, the IO programs in each round must wait for these CPU-bound processes to release the processor. As a consequence, it is effectively observed in the graph that if we fix a case with CPU-bound processes and decrease its quantum, its performance improves, as these CPU-bound processes release the processor earlier, allowing IO-bound programs to perform better.

#### Summary

- Decreasing the quantum worsens the performance of cases that only have iobenchs and improves the cases that mix iobench with cpubench (Except for scenario 4).
- The highest amount of IOPT obtained was in case 0 with the default quantum.
- The lowest amount of IOPT obtained was in case 2 with quantum/1000 (Scenario 3).
- The scenario with the best average is scenario 2 (quantum/100) with the following results:
  ```
        Scenario 0: 1742.6709602767417
      Scenario 1: 2025.1486125378076
      Scenario 2: 2303.985178609142
      Scenario 3: 1085.7201714348455
  ```

### Analysis of CPU Graph

![IOBENCH Metrics RR](https://media.discordapp.net/attachments/882386883643056129/902297312250974299/captura_3.png?width=685&height=397)

_Clarification:
In this part of the analysis, we chose to divide the graph into 2 because scenario 3 cannot be appreciated due to the scales used._

When analyzing the results of cpubench for RoundRobin, one thing becomes clear: the best scenario is the one with the default quantum, that is, the one with the highest quantum.
The difference is quite visible, for example, the performance is 10 times higher in the default quantum compared to the quantum 10 times smaller.
In the case where we only have one cpubench running, the default quantum averages 577 Kflops, while for a quantum 10 times smaller, it averages 53 Kflops. Then, for 100 times smaller, it is 2.4 Kflops, and finally, the scenario with a quantum 1000 times smaller averages just 1 Kflop.
What we are seeing here is that clearly as we decrease the quantum, the performance of cpubench decreases.
This happens because the larger the quantum, the scheduler behaves more and more like a FIFO, which is very beneficial for CPU-bound processes (i.e., those that spend most of their execution time using the processor). If the quantum is the time I will lend to a process the CPU before taking it away, the longer this time, the better its performance.
The result then is quite intuitive, as we decrease this quantum throughout the scenarios, we see how cpubench starts to have worse and worse performance, to the point that for a thousand times smaller quantum, its result is very difficult to see in the graph, and we have to resort to a second auxiliary graph with another scale.
Another thing we can take from these results is how cpubench behaves along with other processes. Specifically, when it has other cpubench or iobench running in parallel.
As can be seen in the graph, the best result for cpubench is obtained when there is no other cpubench running simultaneously. That is when it either does not have to share the CPU, or it has to do so with iobench.
This is evident in the first, third, and fourth cases, where none of them have more than one cpubench running in parallel. Then, in the rest of the cases, we see that the performance of cpubench drops to just over half.
Why does this happen?
Simply put, when cpubench has to share CPU time with iobench, it does not have much of a problem, as iobench is an IO-bound process, which means it will occupy little CPU time and can quickly return the processor to cpubench to continue executing until its quantum is over.
It is different when we run two cpubench in parallel. When this happens, we have two CPU-bound processes sharing the processor. Both will want to use all their quantum, so one must wait for the other to finish before continuing. One way to see it is that if we have two CPU-bound processes, we have to divide the CPU time in two, and we don't worry so much about IO-bound processes because they are not very interested in using much CPU time. This is why when running two cpubench in parallel, for all scenarios with different quantums, the performance decreases to a value close to half.

#### Summary:

In conclusion, we can say that for CPU-bound processes, the quantum is very important. It's easy to think that these processes will improve their performance if we give them more CPU time, and worsen if we give them less. Additionally, if we have multiple CPU-bound processes, their performance will worsen when they have to share CPU time, unlike when they have to share it with IO-bound processes that won't "steal" their precious CPU time. Both conclusions are observable in the graphs.

## Third Part

### Exercise 1:

For this exercise, the **prio** field was added to the process structure, where a process priority ranging from {0,1,2} will be stored as follows:

```c
#define NPRIO 3

// Per-process state
struct proc {
  uint sz;                     // Size of process memory (bytes)
  pde_t* pgdir;                // Page table
  char *kstack;                // Bottom of kernel stack for this process
  enum procstate state;        // Process state
  int pid;                     // Process ID
  struct proc *parent;         // Parent process
  struct trapframe *tf;        // Trap frame for current syscall
  struct context *context;     // swtch() here to run process
  void *chan;                  // If non-zero, sleeping on chan
  int killed;                  // If non-zero, have been killed
  struct file *ofile[NOFILE];  // Open files
  struct inode *cwd;           // Current directory
  char name[16];               // Process name (debugging)
  int prio;                    // Process scheduler priority
};
```

Our next goal is to ensure that a new process is assigned the highest priority `prio = 0` upon initiation. To achieve this, we located where processes were created, leading us to the `allocproc()` function in the **proc.c** file. Once there, we added the initialization of the prio state along with the initialization of the process in question. We believed that this was the appropriate place because this function searches the ptable for new processes which are in the UNUSED state, to later be changed to EMBRYO. Therefore, we think this is the optimal place to initialize their priority.

Now, to decrease the priority, the implementation was not very difficult either. However, similar to the previous exercise, we had to pause to consider when, in the program's life, we should lower the priority. Using the image referenced in the second exercise of the first part and the document attached to the most relevant quote, we arrived at the following conclusion:

[A process that wishes to relinquish the CPU calls the function sched. This function triggers a context switch, and when the process is switched back in at a later time, it resumes execution again in sched itself. Thus a call to sched freezes the execution of a process temporarily.

When does a process relinquish its CPU in this fashion? When a timer interrupt occurs and it is deemed that the process has run for too long, the trap function calls yield, which in turn calls sched.

When a process terminates itself using exit, it calls sched one last time to give up the CPU

When a process has to block for an event and sleep, it calls sched to give up the CPU. The function sched simply checks various conditions, and calls swtch to switch to the scheduler thread.]

With this understanding, we implemented within the `yield()` function a condition for those programs with a priority index less than two, such that one point is added to them. It is important to remember that priorities are given in values of {0,1,2} where zero is the highest priority and two is the lowest.

## Forth Part

Having concluded the third part, our processes now have assigned priorities, which will vary depending on how much CPU they use. Processes requesting a lot of CPU time will have their priority lowered, while those needing less will have their priority raised. We have a total of three priority levels (0, 1, and 2), where a lower number indicates higher priority.

The goal of this task is to leverage these priorities to obtain a smarter scheduler that knows how to use priorities to maximize hardware utilization. Thus, the concept of Multilevel Feedback Queue (MLFQ) is born, a scheduling system that executes processes with higher priority first and uses experience to determine a process's priority.

What was resolved in part three addresses how to learn from experience to determine a process's priority. When a process uses all its quantum, it means it's apparently CPU-bound, and it would be beneficial to lower its priority since its execution leads to a loss of system interactivity. Conversely, when a process uses less CPU time than its quantum allows, its priority should be raised, as it appears to be IO-bound, meaning it will mainly engage in input/output activities and won't require much CPU time. These are the processes we want to prioritize since they are interactive processes, and we can afford to wait for them to become blocked during input/output operations before running more CPU-bound processes, which will have lower priority.

We can then think of processes located in different queues according to their assigned priority.

In addition to the rules introduced in part three, two new rules are introduced in this part:

MLFQ rule 1: If process A has a higher priority than process B, A runs (not B).
MLFQ rule 2: If two processes A and B have the same priority, they run in round-robin fashion for the determined quantum.

These rules in their entirety describe the behavior of the MLFQ scheduler.

Great, so we have one question left: How do we implement MLFQ in xv6?

To achieve this, we need to modify the default scheduler, which works with Round-Robin and is found in the proc.c module.

```c
void scheduler(void)
{
..
 int curr_pri = 0;
 for(;;){
   sti();
   acquire(&ptable.lock);
   for(p = ptable.proc; p < &ptable.proc[NPROC]; p++){
     if(p->state != RUNNABLE || p->prio != curr_pri){
```

Now, when iterating through the process table, we discard those processes that are either not runnable or do not have the desired priority.

If we find a process with priority 0, it will be executed. But what happens if we reach the end of the table and haven't found any processes?

Then it's necessary to know where we are in the table while iterating and if we're at the last position because here's where we make a decision: we lower the required priority and iterate again. And if we've gone through all three priorities and found no processes? We need to exit the for loop, release the process table, and start over again.

So, a counter "co" that updates with each iteration of the for loop will help us know our position to make decisions accordingly.

```c
co = 0;
..
   for(p = ptable.proc; p < &ptable.proc[NPROC]; p++){
     co++;
     if(p->state != RUNNABLE || p->prio != curr_pri){
       if(co > 63){
         // We have reached the end of the table
```

If we reach the end of the table, we enter the if statement and can make the decision.

Another important issue: Rule 2.

MLFQ rule 2 says: If two processes A and B have the same priority, they run in round-robin fashion.

To our understanding, when traversing a queue of priority X, we must execute all processes that have arrived at that queue at that moment.

The question we asked ourselves is: What happens after executing a process? Do we refresh the process table and start looking again with priority 0?

The final answer, which fulfills the condition of rule two, is that if we are iterating through the process table with a priority, for example 1, we must run in round-robin fashion all processes that currently (at that acquire of the process table) have priority 1. And once we finish running them, then we update the process table again to search for those in the highest priority (0). This causes processes with the same priority to run in RR, but if a process with a higher priority arrives, we return to the first queue after finishing the RR in the current queue. We only lower the priority if we haven't found any process to execute (runnable) in our current priority queue, but if we find any, we run all those with that priority in RR, and then when we reach the end of the queue, we start all over again, searching from the highest priority.

A solution like this can lead to starvation, meaning that some processes may never run. For example, if we always have processes with priority 0 and 1 in the runnable state in each acquire of the process table, and also have some with priority 2 ready to be executed, these will never get a chance to run.

That's why it was necessary to create a boolean variable "found," which is 0 if no process appeared in the queue or 1 if we have executed at least 1. If we have executed at least 1, it means that if we reach the end of the process table, we must move up because we have finished running the RR for processes of equal priority. If it's 0, it means that in this priority, we haven't found any runnable process, so we must move down to search in lower queues.

So, the code, when executing a process, looks like this:

```
     c->proc = p;
     switchuvm(p);
     p->state = RUNNING;
     swtch(&(c->scheduler), p->context);
     switchkvm();
...
     c->proc = 0;
     found = 1; //Means we have executed at least one proccess on this priority queue;
     continue;
Then, in case we reach the end of the process table, we make the decision:



     if(p->state != RUNNABLE || p->prio != curr_pri){
       if(co > 63){
         //We have reached the end of the table
         co = 0;
         if (found){
         //We ran one or multiple proccesses on this priority, so we should go back to first priority queue
             found = 0;
             curr_pri = 0;
             goto end; //Exit and re-acquire ptable
         }
         //We did not enconter any process in this priority, so we must search on lower queues
         p = ptable.proc;
         curr_pri++;

```

If found is not true, then we must iterate again in the table, but this time searching with a lower priority.

Let's remember that this MLFQ implementation can lead to starvation, as we could be executing processes from the highest priorities and never reach the lower queues. One possible solution to this problem is to periodically increase the priority of all processes, which is known as a Priority Boost. This way, all processes will have high priority at some point in time and will be executed.

Adding some extra bits to the code, this implementation ensures the two missing rules of MLFQ:

- Because we first search for and execute processes in the runnable state with higher priority, and only search in lower priority queues if we don't find anything in the higher ones, it fulfills the rule that if process A has a higher priority than process B, A runs (and not B).
- And because if we find a process in a queue, then we finish running that queue before starting all over again with the highest priority, it fulfills the rule that if two processes A and B have the same priority, they run in round-robin fashion for the determined quantum.

This concludes the MLFQ implementation.

## MLFQ Results and Comparison with RR

![IOBENCH metrics for MLFQ](https://media.discordapp.net/attachments/879827579471818762/902382757563486208/unknown.png?width=713&height=397)

![CPUBENCH metrics for MLFQ](https://media.discordapp.net/attachments/879827579471818762/902382896243961867/unknown.png?width=730&height=397)

At this point, we observed that the expected outcome was not reflected in the experiment results. The result that is evident at first glance is: There was no significant change.
What would have made sense to find here are graphs showing better performance for MLFQ compared to RR. Specifically, we expected better performance from iobench in those cases where iobench and cpubench processes run in parallel. This intuition stems from the fact that the priority system would allow iobench to perform better, as it prioritizes IO-bound processes.
Faced with this inconsistency, we propose different possible causes:

- The first idea is that this could stem from a flawed implementation of MLFQ. Although we believe that our implementation of MLFQ is correct, this type of code is prone to errors. Therefore, a solution to this problem would be a thorough review of the implemented algorithm.

- Another possible cause is the hardware. Remember that xv6 is being emulated and does not run natively on the computer, so there could be inconsistencies that might not appear if xv6 were run natively. Additionally, while attempts were made to run the tests in as controlled an environment as possible (meaning that each time the tests were run, an attempt was made to leave the system in the same state, with the same processes running), there may have been some determining factor that affected the performance of the tests while xv6 was being emulated.

- A final possible cause is the experiments used. Although we strongly believe that the experimental model used in this project is correct and appropriate, we should not rule out the possibility that the different experiments used in this lab may not be sufficient to appreciate the effect of changing the scheduler. The solution to this problem might be to try other experiments.

## Final Conclusion

In this lab, we were able to observe the operation of the scheduler in a real operating system and the result of modifying this scheduler and how processes behave in different scenarios.
In addition to topics related to operating systems and particularly process scheduling, this lab allowed us to delve into the world of data science, as we had to automate processes for manipulating information and conduct in-depth analysis with this information.
One of the most challenging parts was, as has been the case with previous labs, understanding the code, as there are many modules and functions that are not immediately clear in their purpose and require careful analysis to gain a deeper understanding. The part of devising the algorithm for the MLFQ system was also difficult, as small changes would break xv6 and the debugging process was very difficult.
We conclude the lab by presenting results that align with what was expected based on the theory, and some results that may differ slightly, which may be due to the aforementioned causes.
This lab has been very interesting because it allowed us to see more deeply into the operation of the scheduler in an operating system like xv6, and it allowed us to get involved in its code, which is something that is not possible in the theoretical part of the course.
